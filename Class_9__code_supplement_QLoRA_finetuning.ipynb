{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class 9 code supplement: Teaching Falcon7B to write better haikus using QLoRA\n",
        "\n",
        "This notebook illustrates the process of performing paramater-efficient finetuning (PEFT) of the Falcon7B model using the a dataset of prompts that instruct the model to write haikus on different subjects."
      ],
      "metadata": {
        "id": "CLv-uYtlT2Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install needed libraries: `peft,` `bitsandbytes`, `accelerate`, and `trl`\n",
        "\n",
        "The `peft` library contains Hugging Face implementations of various PEFT techniques, including LoRA. `bitsandbytes` will allow us to run Falcon in 4bit precision and `accelerate` has handy model loading utilities and also simplifies the process of finetuning in a distributed fashion. The `trl` library contains convenient classes for supervised finetuning."
      ],
      "metadata": {
        "id": "uml3wgdri2_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UyyuMGnCPjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61020ad-e514-4b94-f24e-0703fcd79290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/179.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.1.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q fsspec==2024.10.0\n",
        "!pip install -q accelerate==0.29.3\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q trl==0.8.6\n",
        "!pip install -q peft==0.10.0\n",
        "!pip install -q transformers==4.41.0\n",
        "!pip install -q wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch"
      ],
      "metadata": {
        "id": "pBOE-h8sbrNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EHzVpaYMfVde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Specify the Hugging Face model and the target modules\n",
        "We'll be using Falcon7B for this task. Although this is a very low-performing model, it has the benefit of being fairly small and manageable. We also specify the target modules that we want to finetune in Falcon. Finetunable target modules differ by model type. For a list of common LLMs and their target modules see [here](https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)."
      ],
      "metadata": {
        "id": "cwAiEFifgp3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vziwd2UuCYGl"
      },
      "outputs": [],
      "source": [
        "model_name = \"tiiuae/falcon-7b\"\n",
        "target_modules = [\"query_key_value\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Configure the 4-bit quantization logic and load the quantized model\n",
        "\n",
        "To load the model, we need a configuration class that specifies how we want the quantization to be performed. We'll use `BitesAndBytesConfig` from the `Transformers` library for this. We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss. We can then load the quantized model into memory."
      ],
      "metadata": {
        "id": "UVF_hKiPd1lh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h_ydWGf6EAd"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ea9b6743244344a7bb3e8cc4b605dbbb",
            "fbfc26341893434ea2ed980d3a39da79",
            "74533acfc7e042c08f341d3f8eb0e389",
            "7ee3559240c5458581d0260a71ed140e",
            "ccca8c6b1be7481181239e95b26eb6bd",
            "b227b12bb53b4edda61fad1fa10c5838",
            "98a828f2ee744ebbb43fcaa51501e0f5",
            "6ebaaf432d334bf6a7c5c4b0e0e96fa6",
            "d9844e2cedbb490b90a1e6612aa3c39a",
            "89b0131591944f7f816ea7011e26fac0",
            "3b5e2dd47ed94cfc88732603676db604"
          ]
        },
        "id": "W2EZhNQ66EAd",
        "outputId": "467c73f3-9052-4e51-c8e5-45913942a865"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea9b6743244344a7bb3e8cc4b605dbbb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "device_map = {\"\": 0}\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                    quantization_config=bnb_config,\n",
        "                    device_map=device_map,\n",
        "                    use_cache = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "aU0awofs84q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Step 4: Examine output from the base (non-finetuned) model\n",
        "\n",
        "The point of this exercise is to improve an LLM's ability to perform a particular task---in this case composing haikus. Let's first examine how Falcon does on this task without any finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jak6FzpvFTHk"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=False, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAYg7czFYeK",
        "outputId": "d501a96d-d479-475d-a6c9-cc77686e68d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Write a haiku that captures the beauty of new york city.\\nI’m not sure if I’m doing this right, but here goes:\\nNew York City\\nThe city that never sleeps\\nA place to be seen\\nA place to be heard\\nA place to be seen\\nA place to be heard\\nA place to be seen\\nA place to be heard\\nA place to be seen\\nA place to be heard\\nA place to be seen\\nA place to be heard\\nA place to be seen\\nA place to be heard']\n"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "input_sentences = tokenizer(\"Write a haiku that captures the beauty of new york city.\", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=100)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "The LLM's haiku is quite bad--unimaginitive and full of repetitions. Let's teach it to be better!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "## Step 5: Preparing the Dataset\n",
        "Next, we pull a dataset of haiku prmpts from [here](https://huggingface.co/datasets/davanstrien/haiku_prompts/), load it, and take a 50-prompt sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "DyIMQ7IHFbIx",
        "outputId": "27b89225-cf92-4d11-873f-046ac573f101"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instructions', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"davanstrien/haiku_prompts\"\n",
        "\n",
        "#Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"instructions\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "\n",
        "del data\n",
        "#train_sample = train_sample.remove_columns('tags')\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlZY3fk_9fm",
        "outputId": "6cb03ccf-0abe-43a3-8ec0-6e9d437b2e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instructions': ['Can you compose a haiku about the serenity of mountain peaks?', 'Write a haiku that captures the grandeur of mountain peaks.', 'Please create a haiku that describes the solitude of mountain peaks.', 'Can you write a haiku about the mystery of mountain peaks?', 'Compose a haiku that conveys the majesty of mountain peaks.', 'Please create a haiku that portrays the tranquility of mountain peaks.', 'Can you write a haiku that evokes the awe-inspiring feeling of mountain', 'Can you write a haiku about moss growing on a tree trunk?', 'Please compose a haiku that captures the serenity of moss.', 'Create a haiku that describes the texture of moss.'], 'input_ids': [[5160, 299, 30301, 241, 374, 26916, 544, 248, 55878, 275, 8383, 26688, 42], [12081, 241, 374, 26916, 325, 24675, 248, 58081, 275, 8383, 26688, 25], [5000, 1849, 241, 374, 26916, 325, 11117, 248, 45423, 275, 8383, 26688, 25], [5160, 299, 3131, 241, 374, 26916, 544, 248, 12738, 275, 8383, 26688, 42], [2349, 2975, 241, 374, 26916, 325, 52637, 248, 64398, 275, 8383, 26688, 25], [5000, 1849, 241, 374, 26916, 325, 55343, 248, 58364, 275, 8383, 26688, 25], [5160, 299, 3131, 241, 374, 26916, 325, 59477, 248, 24541, 24, 57934, 3475, 275, 8383], [5160, 299, 3131, 241, 374, 26916, 544, 38352, 4040, 313, 241, 4950, 21318, 42], [5000, 30301, 241, 374, 26916, 325, 24675, 248, 55878, 275, 38352, 25], [9406, 241, 374, 26916, 325, 11117, 248, 10504, 275, 38352, 25]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "## Step 6: Fine-tuning\n",
        "Finetuning starts with the creation of a LoRA configuration object containing the specified settings for finetuning,\n",
        "\n",
        "* `r` This parameter specifies how many parameters will be trained. This can be tuned based on the complexity of the task.\n",
        "\n",
        "* `lora_alpha` Adjust this to increase or decrease the weights of the LoRA activations. Larger values increases the impact of the finetuning data but too large values lead to catastrophic forgetting\n",
        "\n",
        "* `lora_dropout` This is similar to the dropout regularization procedure used in general neural network training\n",
        "\n",
        "* `bias` A rule of thumb for this parameter: For text classification the most common value is `none`, and for chat or question answering, `all` or `lora_only`\n",
        "\n",
        "* `task_type` Indicates the task the model is being finetuned for. In this case, the task is text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCalslQFGL7K"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16, #larger values = more parameters to train.\n",
        "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05, #Helps to avoid overfitting.\n",
        "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the model\n",
        "import os\n",
        "working_dir = 'sample_data' #assumes you're on colab in the same directory as `sample_data\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND0aJ-t6ARqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea819860-a879-4237-d137-ac16622253da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct baatch size that fits the size of Data.\n",
        "    learning_rate= 2e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=5,\n",
        "    report_to=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we're ready to do our finetuning with the following elements:\n",
        "\n",
        "* The model.\n",
        "* The training_args\n",
        "* The sataset\n",
        "* The result of `DataCollator`, the dataset ready to be procesed in blocks.\n",
        "* The LoRA config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "z5NYHqBnGZyF",
        "outputId": "6cbe3e88-e792-4856-ec1d-a2c7c9deaecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:08, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=35, training_loss=2.7450413295200895, metrics={'train_runtime': 8.9138, 'train_samples_per_second': 28.046, 'train_steps_per_second': 3.927, 'total_flos': 193677378769920.0, 'train_loss': 2.7450413295200895, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "import osgeo\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    peft_config = lora_config,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEKiFdpDGgOx"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model.\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "s6Hmy87-HViP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ca9b09-c820-4f70-bd3b-33400508e945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In case you are having memory problems uncomment this lines to free some memory\n",
        "import gc\n",
        "import torch\n",
        "del foundation_model\n",
        "del trainer\n",
        "del train_sample\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "glDE0Z4FOSe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a29b75a-a8d7-4151-ec1a-562beeaf67fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Load the PEFT model and examine the change in output quality\n",
        "Now that we've finetuned and saved our PEFT model, we're ready to load it and see if it has improved it haiku-writing abilities"
      ],
      "metadata": {
        "id": "UOAqEg0mSjHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "working_dir = 'sample_data'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ],
      "metadata": {
        "id": "dX5d8xMCSC6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "CA-E-io_Dfe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TAjrSWSe14q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5b2fa8aac2e24b98b722a553179a9acd",
            "d89fd250b63746b5888dd68f98c95295",
            "01320e5283ec4240af3a104b01dcb66c",
            "71f74359ab344145879ae982c99223bb",
            "4668dc1f7ff0490a988b2cf6427fc300",
            "0e5022a7c5ec4e79b23ff8cb8c7feaa2",
            "209f60af940a4855b30d3cc9fbc43ce8",
            "fb459b7a17904245bf285b078c538ba8",
            "99b28c1823414e36bb729dd6d4d3c96e",
            "d7570194b179465faf9fbaf1aca7caa4",
            "214c88f1e3724a83a25ca0f3ab1a6e54"
          ]
        },
        "outputId": "641b6c4d-dfbf-4ac7-d814-f053458a9fb9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b2fa8aac2e24b98b722a553179a9acd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_27uvJudf03",
        "outputId": "d7464acd-25c3-4aee-af2b-4945ace7cfe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Write a haiku that captures the beauty of new york city.\\nNew York City\\nThe city of lights\\nA place to be seen\\nA place to be heard\\nA place to be free\\nA place to be loved\\nA place to be hated\\nA place to be feared\\nA place to be respected\\nA place to be hated\\nA place to be feared\\nA place to be respected\\nA place to be loved\\nA place to be hated\\nA place to be feared\\nA place to be respected\\nA place to be loved\\n']\n"
          ]
        }
      ],
      "source": [
        "input_sentences = tokenizer(\"Write a haiku that captures the beauty of new york city.\", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=100)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better! No repititions and much more creative..."
      ],
      "metadata": {
        "id": "hDPrFYqGcS_u"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea9b6743244344a7bb3e8cc4b605dbbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbfc26341893434ea2ed980d3a39da79",
              "IPY_MODEL_74533acfc7e042c08f341d3f8eb0e389",
              "IPY_MODEL_7ee3559240c5458581d0260a71ed140e"
            ],
            "layout": "IPY_MODEL_ccca8c6b1be7481181239e95b26eb6bd"
          }
        },
        "fbfc26341893434ea2ed980d3a39da79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b227b12bb53b4edda61fad1fa10c5838",
            "placeholder": "​",
            "style": "IPY_MODEL_98a828f2ee744ebbb43fcaa51501e0f5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "74533acfc7e042c08f341d3f8eb0e389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebaaf432d334bf6a7c5c4b0e0e96fa6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9844e2cedbb490b90a1e6612aa3c39a",
            "value": 2
          }
        },
        "7ee3559240c5458581d0260a71ed140e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89b0131591944f7f816ea7011e26fac0",
            "placeholder": "​",
            "style": "IPY_MODEL_3b5e2dd47ed94cfc88732603676db604",
            "value": " 2/2 [00:08&lt;00:00,  3.95s/it]"
          }
        },
        "ccca8c6b1be7481181239e95b26eb6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b227b12bb53b4edda61fad1fa10c5838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a828f2ee744ebbb43fcaa51501e0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ebaaf432d334bf6a7c5c4b0e0e96fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9844e2cedbb490b90a1e6612aa3c39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89b0131591944f7f816ea7011e26fac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5e2dd47ed94cfc88732603676db604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b2fa8aac2e24b98b722a553179a9acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d89fd250b63746b5888dd68f98c95295",
              "IPY_MODEL_01320e5283ec4240af3a104b01dcb66c",
              "IPY_MODEL_71f74359ab344145879ae982c99223bb"
            ],
            "layout": "IPY_MODEL_4668dc1f7ff0490a988b2cf6427fc300"
          }
        },
        "d89fd250b63746b5888dd68f98c95295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e5022a7c5ec4e79b23ff8cb8c7feaa2",
            "placeholder": "​",
            "style": "IPY_MODEL_209f60af940a4855b30d3cc9fbc43ce8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "01320e5283ec4240af3a104b01dcb66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb459b7a17904245bf285b078c538ba8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99b28c1823414e36bb729dd6d4d3c96e",
            "value": 2
          }
        },
        "71f74359ab344145879ae982c99223bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7570194b179465faf9fbaf1aca7caa4",
            "placeholder": "​",
            "style": "IPY_MODEL_214c88f1e3724a83a25ca0f3ab1a6e54",
            "value": " 2/2 [00:09&lt;00:00,  4.19s/it]"
          }
        },
        "4668dc1f7ff0490a988b2cf6427fc300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e5022a7c5ec4e79b23ff8cb8c7feaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "209f60af940a4855b30d3cc9fbc43ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb459b7a17904245bf285b078c538ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b28c1823414e36bb729dd6d4d3c96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7570194b179465faf9fbaf1aca7caa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214c88f1e3724a83a25ca0f3ab1a6e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}